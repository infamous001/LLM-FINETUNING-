# LLM-FINETUNING-
Instruct tuned a open tuned a open source LLM and fine tuned using LORA

DATASET USED ->DOLLY 15K BY DATABRICKS 
databricks-dolly-15k is a corpus of more than 15,000 records generated by thousands of Databricks employees to enable large language models to exhibit the magical interactivity of ChatGPT. Databricks employees were invited to create prompt / response pairs in each of eight different instruction categories, including the seven outlined in the InstructGPT paper, as well as an open-ended free-form category. The contributors were instructed to avoid using information from any source on the web with the exception of Wikipedia (for particular subsets of instruction categories), and explicitly instructed to avoid using generative AI in formulating instructions or responses. Examples of each behavior were provided to motivate the types of questions and instructions appropriate to each category.

This dataset is used for instruct finetuning the data using hugging face library
link to dataset-> https://huggingface.co/datasets/databricks/databricks-dolly-15k

MODEL USED ->OPENLLAMA-7B
This model is a reproduction of Meta AI's LLaMA large language model.It conatin 7b model
